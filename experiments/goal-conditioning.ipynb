{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal-oriented self-improvement for Automated Theorem Proving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method\n",
    "We are basically thinking of extending minimo by introducing a goal i.e. a set of problems the model should eventually solve. We do this by adding loss-term forcing the model to sample conjectures from our goal set. We control this ‘forcing’ using a hyperparameter `alpha`. A higher value gives the `progess_loss` more value, thus pushes the model towards the goal stronger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dockerfile\t\t slurm-1278750.out  slurm-1279229.out\n",
      "environment\t\t slurm-1278751.out  slurm-1279230.out\n",
      "experiments\t\t slurm-1278752.out  slurm-1279231.out\n",
      "FAQ.md\t\t\t slurm-1278753.out  slurm-1279344.out\n",
      "goals\t\t\t slurm-1278754.out  slurm-1279345.out\n",
      "install_redis.sh\t slurm-1278755.out  slurm-1279346.out\n",
      "launch\t\t\t slurm-1278756.out  slurm-1279347.out\n",
      "learning\t\t slurm-1278757.out  slurm-1279350.out\n",
      "LICENSE\t\t\t slurm-1278758.out  slurm-1279351.out\n",
      "outputs\t\t\t slurm-1278759.out  slurm-1279352.out\n",
      "pyproject.toml\t\t slurm-1278760.out  slurm-1279353.out\n",
      "README.md\t\t slurm-1278761.out  slurm-1279387.out\n",
      "redis_hostname_port.txt  slurm-1278762.out  slurm-1279388.out\n",
      "setup.sh\t\t slurm-1278978.out  slurm-1279389.out\n",
      "slurm-1278655.out\t slurm-1278979.out  slurm-1279390.out\n",
      "slurm-1278737.out\t slurm-1278980.out  slurm-1279502.out\n",
      "slurm-1278739.out\t slurm-1278981.out  slurm-1279503.out\n",
      "slurm-1278740.out\t slurm-1278982.out  slurm-1279504.out\n",
      "slurm-1278742.out\t slurm-1278983.out  slurm-1279505.out\n",
      "slurm-1278743.out\t slurm-1278984.out  slurm-1279506.out\n",
      "slurm-1278744.out\t slurm-1278985.out  slurm-1279507.out\n",
      "slurm-1278745.out\t slurm-1279188.out  slurm-1279508.out\n",
      "slurm-1278747.out\t slurm-1279192.out  slurm-1279509.out\n",
      "slurm-1278748.out\t slurm-1279193.out  tutorial.md\n",
      "slurm-1278749.out\t slurm-1279228.out  wandb\n"
     ]
    }
   ],
   "source": [
    "# lets move to the parent directory so it is easier to run the scripts\n",
    "import os\n",
    "\n",
    "if os.getcwd().split('/')[-1] == 'experiments':\n",
    "    os.chdir('../')\n",
    "\n",
    "!ls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 1276568\n",
      "Submitted batch job 1276569\n",
      "Submitted batch job 1276570\n"
     ]
    }
   ],
   "source": [
    "# Let's start some workers so our tasks get executed in parallel. \n",
    "# @franz you would need to start them on the same node\n",
    "!sbatch --job-name=redis --cpus-per-task=10 --mem=250G --time=20:00:00 --wrap=\"./launch/start_redis.sh -v\"\n",
    "\n",
    "!sbatch --job-name=worker1 --cpus-per-task=3 --gres=gpu:1 --mem=20G --time=20:00:00 --wrap=\"./launch/start_worker.sh\"\n",
    "!sbatch --job-name=worker2 --cpus-per-task=3 --gres=gpu:1 --mem=20G --time=20:00:00 --wrap=\"./launch/start_worker.sh\"\n",
    "# !sbatch --job-name=worker3 --cpus-per-task=3 --gres=gpu:1 --mem=20G --nodelist=node[6] --time=20:00:00 --wrap=\"./launch/start_worker.sh\"\n",
    "# !sbatch --job-name=worker4 --cpus-per-task=3 --gres=gpu:1 --mem=50G --nodelist=node[6] --time=20:00:00 --wrap=\"./launch/start_worker.sh\"\n",
    "# !sbatch --job-name=worker5 --cpus-per-task=3 --gres=gpu:1 --mem=50G --nodelist=node[6] --time=20:00:00 --wrap=\"./launch/start_worker.sh\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the now we use a very simple goal set. It consists of a single goal which is proving the theorem: \n",
    "\n",
    "`a is a natural number: (0 + a) = a`\n",
    "\n",
    "Or in peano:\n",
    "\n",
    "`[('a0 : nat) -> (= (+ z 'a0) 'a0)]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Overfit on the goal set\n",
    "After implementing the goal-conditioning, let's run experiments with `alpha=0` and `alpha=1`. We expect the `progress_loss` to go down i.e. the model overfits to the `final_goal` set. However, the actual `train_loss` shouldn’t go down. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 1275940\n",
      "Submitted batch job 1275941\n"
     ]
    }
   ],
   "source": [
    "# start a job with alpha=1 to overfit on the goals\n",
    "!sbatch --job-name=train_alpha_1 --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=1:00:00 --wrap=\"./launch/run_bootstrap_distributed.sh agent.max_mcts_nodes=100 agent.policy.alpha=1 agent.policy.total_iterations=2\"\n",
    "\n",
    "# start a job with alpha=0 as a baseline\n",
    "!sbatch --job-name=train_alpha_0 --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=1:00:00 --wrap=\"./launch/run_bootstrap_distributed.sh agent.max_mcts_nodes=100 agent.policy.alpha=0 agent.policy.total_iterations=2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Works! The ``training_loss`` diverges for `alpha=1`. The `progress_loss` struggles to go below a certain threshold for `alpha=0`. As per our intuition, the former overfits to sampling the final theorem and can't solve any conjectures in the second iteration. We need something better for alpha."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Try different values for alpha\n",
    "\n",
    "We try different schedules for alpha in the hope that the training loss and progress loss both converge. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 1275099\n"
     ]
    }
   ],
   "source": [
    "# TODO this still runs on single GPU\n",
    "!sbatch --job-name=train_alpha_0_8 --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=24:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add alpha=0.8\"\n",
    "\n",
    "!sbatch --job-name=train_alpha_0_6 --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=24:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add alpha=0.6\"\n",
    "\n",
    "!sbatch --job-name=train_alpha_0_4 --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=24:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add alpha=0.4\"\n",
    "\n",
    "!sbatch --job-name=train_alpha_0_2 --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=24:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add alpha=0.2\"\n",
    "\n",
    "!sbatch --job-name=train_alpha_0_3e_4 --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=24:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add alpha=3e-4\"\n",
    "\n",
    "!sbatch --job-name=train_alpha_0_1e_3 --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=24:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add alpha=1e-3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works as well. We also observe a correlation between alpha values and the ratio of conjectures that the model is able to prove per iteration. The higher the alpha value, the fewer problems the model can solve. What if we could let the model explore problems for itself for a while and at a later stage push it towards our goal?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Alpha Schedules\n",
    "\n",
    "We implement fancy schedules to 'warm-up' alpha over several iterations. The options are \n",
    "\n",
    "`alpha_schedule = [ constant | linear | quadratic | cubic | cos ]`\n",
    "\n",
    "#### 3.1 Warm up alpha to 1.0\n",
    "\n",
    "As a first step we warm up alpha to 1.0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 1274969\n"
     ]
    }
   ],
   "source": [
    "# TODO this still runs on single GPU\n",
    "!sbatch --job-name=train_alpha_lin --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=10:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add alpha=1 alpha_schedule=linear\"\n",
    "\n",
    "!sbatch --job-name=train_alpha_quad --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=10:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add alpha=1 alpha_schedule=quadratic\"\n",
    "\n",
    "!sbatch --job-name=train_alpha_cubic --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=10:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add alpha=1 alpha_schedule=cubic\"\n",
    "\n",
    "!sbatch --job-name=train_alpha_cos --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=10:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add alpha=1 alpha_schedule=cos\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is apparent that high values of alpha just overpower the actual loss. Ideally we wan't to keep alpha much smaller. Let's try just warming it up to lower values. \n",
    "\n",
    "#### 3.2 Warm up alpha to lower values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 1274557\n",
      "Submitted batch job 1274558\n",
      "Submitted batch job 1274559\n"
     ]
    }
   ],
   "source": [
    "# TODO this still runs on single GPU\n",
    "!sbatch --job-name=train_alpha_cubic --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=10:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add alpha=0.2 alpha_schedule=cubic\"\n",
    "\n",
    "!sbatch --job-name=train_alpha_cubic --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=10:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add alpha=0.3 alpha_schedule=cubic\"\n",
    "\n",
    "!sbatch --job-name=train_alpha_cubic --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=10:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add alpha=0.4 alpha_schedule=cubic\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 Use the ratio of solved conjectures \n",
    "\n",
    "Now comes the actually interesting part. We implement a more principled schedule.\n",
    "Let's use the ratio of solved conjectures to total sampled conjectures to directly control alpha. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 1274560\n",
      "Submitted batch job 1274561\n"
     ]
    }
   ],
   "source": [
    "# TODO this still runs on single GPU\n",
    "!sbatch --job-name=train_alpha_ratio --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=10:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add alpha=1 alpha_schedule=ratio\"\n",
    "\n",
    "!sbatch --job-name=train_alpha_ratio --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=10:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add alpha=0.2 alpha_schedule=ratio\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 1276645\n",
      "Submitted batch job 1276646\n",
      "Submitted batch job 1276647\n"
     ]
    }
   ],
   "source": [
    "# start a job with alpha schedule ratio and max_alpha=1\n",
    "!sbatch --job-name=train_ratio --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=10:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add agent.policy.alpha=1 agent.policy.alpha_schedule=ratio goals=nat-add-synthetic n_conjectures=50 agent.max_mcts_nodes=200 agent.policy.total_iterations=1000 agent.policy.train_iterations=500\"\n",
    "\n",
    "\n",
    "# start a job with alpha schedule ratio and max_alpha=5e-3\n",
    "!sbatch --job-name=train_ratio_small --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=10:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add agent.policy.alpha=5e-3 agent.policy.alpha_schedule=ratio goals=nat-add-synthetic n_conjectures=50 agent.max_mcts_nodes=200 agent.policy.total_iterations=1000 agent.policy.train_iterations=500\"\n",
    "\n",
    "# start a job with alpha=0 as a baseline\n",
    "!sbatch --job-name=train_vanilla --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=10:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add agent.policy.alpha=0 goals=nat-add-synthetic n_conjectures=50 agent.max_mcts_nodes=200 agent.policy.total_iterations=1000 agent.policy.train_iterations=500\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 1276686\n",
      "Submitted batch job 1276687\n"
     ]
    }
   ],
   "source": [
    "# constants\n",
    "\n",
    "# # start a job with alpha schedule const and max_alpha=0.1e-2\n",
    "# !sbatch --job-name=train_constant_1e_2 --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=10:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add agent.policy.alpha=1e-2 agent.policy.alpha_schedule=constant goals=nat-add-synthetic n_conjectures=50 agent.max_mcts_nodes=200 agent.policy.total_iterations=1000 agent.policy.train_iterations=500\"\n",
    "\n",
    "# # start a job with alpha schedule const and max_alpha=0.1e-3\n",
    "# !sbatch --job-name=train_constant_1e_3 --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=10:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add agent.policy.alpha=1e-3 agent.policy.alpha_schedule=constant goals=nat-add-synthetic n_conjectures=50 agent.max_mcts_nodes=200 agent.policy.total_iterations=1000 agent.policy.train_iterations=500\"\n",
    "\n",
    "# start a job with alpha schedule const and max_alpha=0.1e-4\n",
    "# !sbatch --job-name=train_constant_1e_4 --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=10:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add agent.policy.alpha=1e-4 agent.policy.alpha_schedule=constant goals=nat-add-synthetic n_conjectures=50 agent.max_mcts_nodes=200 agent.policy.total_iterations=1000 agent.policy.train_iterations=500\"\n",
    "\n",
    "# start a job with alpha schedule const and max_alpha=0.1e-5\n",
    "!sbatch --job-name=train_constant_1e_4 --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=10:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add agent.policy.alpha=1e-5 agent.policy.alpha_schedule=constant goals=nat-add-synthetic n_conjectures=50 agent.max_mcts_nodes=200 agent.policy.total_iterations=1000 agent.policy.train_iterations=500\"\n",
    "\n",
    "# start a job with alpha schedule const and max_alpha=0.1e-6\n",
    "!sbatch --job-name=train_constant_1e_4 --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=10:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add agent.policy.alpha=1e-6 agent.policy.alpha_schedule=constant goals=nat-add-synthetic n_conjectures=50 agent.max_mcts_nodes=200 agent.policy.total_iterations=1000 agent.policy.train_iterations=500\"\n",
    "\n",
    "# ratio\n",
    "# start a job with alpha schedule ratio and max_alpha=0.1e-2\n",
    "# !sbatch --job-name=train_ratio1e_2 --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=10:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add agent.policy.alpha=1e-2 agent.policy.alpha_schedule=ratio goals=nat-add-synthetic n_conjectures=50 agent.max_mcts_nodes=200 agent.policy.total_iterations=1000 agent.policy.train_iterations=500\"\n",
    "\n",
    "# # start a job with alpha schedule ratio and max_alpha=0.1e-3\n",
    "# !sbatch --job-name=train_ratio1e_3 --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=10:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add agent.policy.alpha=1e-3 agent.policy.alpha_schedule=ratio goals=nat-add-synthetic n_conjectures=50 agent.max_mcts_nodes=200 agent.policy.total_iterations=1000 agent.policy.train_iterations=500\"\n",
    "\n",
    "# # start a job with alpha schedule ratio and max_alpha=0.1e-4\n",
    "# !sbatch --job-name=train_ratio1e_4 --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=10:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add agent.policy.alpha=1e-4 agent.policy.alpha_schedule=ratio goals=nat-add-synthetic n_conjectures=50 agent.max_mcts_nodes=200 agent.policy.total_iterations=1000 agent.policy.train_iterations=500\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 1276208\n",
      "Submitted batch job 1276209\n"
     ]
    }
   ],
   "source": [
    "# start a job with alpha schedule ratio and max_alpha=1\n",
    "!sbatch --job-name=train_ratio_nl --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --nodelist=node[5] --time=10:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add agent.policy.alpha=1 agent.policy.alpha_schedule=ratio agent.policy.normalize_loss=true goals=nat-add-synthetic n_conjectures=50 agent.max_mcts_nodes=200 agent.policy.total_iterations=1000 agent.policy.train_iterations=500\"\n",
    "\n",
    "# start a job with alpha schedule ratio and max_alpha=5e-3\n",
    "!sbatch --job-name=train_ratio_small_nl --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --nodelist=node[5] --time=10:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add agent.policy.alpha=5e-3 agent.policy.alpha_schedule=ratio agent.policy.normalize_loss=true goals=nat-add-synthetic n_conjectures=50 agent.max_mcts_nodes=200 agent.policy.total_iterations=1000 agent.policy.train_iterations=500\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Increase max iters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 1275039\n",
      "Submitted batch job 1275040\n",
      "Submitted batch job 1275041\n"
     ]
    }
   ],
   "source": [
    "!sbatch --job-name=train_alpha_long --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=20:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add alpha=0 iterations=30\"\n",
    "\n",
    "!sbatch --job-name=train_alpha_long_ratio --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=20:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add alpha=1 alpha_schedule=ratio iterations=30\"\n",
    "\n",
    "!sbatch --job-name=train_alpha_long_cos --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=20:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add alpha=1 alpha_schedule=cubic iterations=30\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. New config and `mu`\n",
    "\n",
    "over the weekend we had a few changes. \n",
    "* we renamed`alpha` to `mu`. \n",
    "* we omit having a fancy `alpha_schedule` and work with a constant `mu` \n",
    "* we set `mu` to zero if the ratio of solved problems goes under a certain `threshold`\n",
    "* we multiply `mu` by the ratio of difficulty-problem pairs to the total training set size. The thought behind this is that we only want to influence the conjecturing. Not the 'solving'.\n",
    "* A couple of bugfixes\n",
    "\n",
    "As a first run let's try out different values of mu and see what happens. For iteration speed we go down to much fewer conjectures and mcts steps. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 1278167\n",
      "Submitted batch job 1278168\n"
     ]
    }
   ],
   "source": [
    "!sbatch --job-name=train_minimo --nodelist=node[5] --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=10:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add n_conjectures=15 agent.max_mcts_nodes=50 agent.policy.batch_size=10000 agent.policy.train_iterations=150 agent.policy.total_iterations=1000 agent.policy.mu=0 goals=nat-add-synthetic\"\n",
    "!sbatch --job-name=train_minimo --nodelist=node[6] --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=10:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add n_conjectures=15 agent.max_mcts_nodes=50 agent.policy.batch_size=10000 agent.policy.train_iterations=150 agent.policy.total_iterations=1000 agent.policy.mu=1 goals=nat-add-synthetic\"\n",
    "!sbatch --job-name=train_minimo --nodelist=node[6] --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=10:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add n_conjectures=15 agent.max_mcts_nodes=50 agent.policy.batch_size=10000 agent.policy.train_iterations=150 agent.policy.total_iterations=1000 agent.policy.mu=0.1 goals=nat-add-synthetic\"\n",
    "!sbatch --job-name=train_minimo --nodelist=node[5] --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=10:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add n_conjectures=15 agent.max_mcts_nodes=50 agent.policy.batch_size=10000 agent.policy.train_iterations=150 agent.policy.total_iterations=1000 agent.policy.mu=0.05 goals=nat-add-synthetic\"\n",
    "!sbatch --job-name=train_minimo --nodelist=node[5] --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=10:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add n_conjectures=15 agent.max_mcts_nodes=50 agent.policy.batch_size=10000 agent.policy.train_iterations=150 agent.policy.total_iterations=1000 agent.policy.mu=0.01 goals=nat-add-synthetic\"\n",
    "!sbatch --job-name=train_minimo --nodelist=node[5] --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=10:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add n_conjectures=15 agent.max_mcts_nodes=50 agent.policy.batch_size=10000 agent.policy.train_iterations=150 agent.policy.total_iterations=1000 agent.policy.mu=0.005 goals=nat-add-synthetic\"\n",
    "!sbatch --job-name=train_minimo --nodelist=node[5] --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=10:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add n_conjectures=15 agent.max_mcts_nodes=50 agent.policy.batch_size=10000 agent.policy.train_iterations=150 agent.policy.total_iterations=1000 agent.policy.mu=0.001 goals=nat-add-synthetic\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ok these runs seem to do very different stuff even though in the beginning (when the prove ratio is below threshold) they should behave very similarly? Is this just randomness? Let's try a single run with different seeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Different seed experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sbatch --job-name=train_minimo --nodelist=node[5] --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=10:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add n_conjectures=15 agent.max_mcts_nodes=50 agent.policy.batch_size=10000 agent.policy.train_iterations=150 agent.policy.total_iterations=1000 agent.policy.mu=0.01 goals=nat-add-synthetic seed=0\"\n",
    "!sbatch --job-name=train_minimo --nodelist=node[5] --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=10:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add n_conjectures=15 agent.max_mcts_nodes=50 agent.policy.batch_size=10000 agent.policy.train_iterations=150 agent.policy.total_iterations=1000 agent.policy.mu=0.01 goals=nat-add-synthetic seed=10\"\n",
    "!sbatch --job-name=train_minimo --nodelist=node[5] --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=10:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add n_conjectures=15 agent.max_mcts_nodes=50 agent.policy.batch_size=10000 agent.policy.train_iterations=150 agent.policy.total_iterations=1000 agent.policy.mu=0.01 goals=nat-add-synthetic seed=15\"\n",
    "!sbatch --job-name=train_minimo --nodelist=node[5] --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=10:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add n_conjectures=15 agent.max_mcts_nodes=50 agent.policy.batch_size=10000 agent.policy.train_iterations=150 agent.policy.total_iterations=1000 agent.policy.mu=0.01 goals=nat-add-synthetic seed=200\"\n",
    "!sbatch --job-name=train_minimo --nodelist=node[5] --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=10:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add n_conjectures=15 agent.max_mcts_nodes=50 agent.policy.batch_size=10000 agent.policy.train_iterations=150 agent.policy.total_iterations=1000 agent.policy.mu=0.01 goals=nat-add-synthetic seed=69\"\n",
    "!sbatch --job-name=train_minimo --nodelist=node[6] --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=10:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add n_conjectures=15 agent.max_mcts_nodes=50 agent.policy.batch_size=10000 agent.policy.train_iterations=150 agent.policy.total_iterations=1000 agent.policy.mu=0.01 goals=nat-add-synthetic seed=420\"\n",
    "!sbatch --job-name=train_minimo --nodelist=node[6] --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=10:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add n_conjectures=15 agent.max_mcts_nodes=50 agent.policy.batch_size=10000 agent.policy.train_iterations=150 agent.policy.total_iterations=1000 agent.policy.mu=0.01 goals=nat-add-synthetic seed=80085\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok so the variance is too high in these experiments (std=0.0625). Let's check if the variance is lower when we increase the number of conjectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 1278290\n",
      "Submitted batch job 1278291\n",
      "Submitted batch job 1278292\n",
      "Submitted batch job 1278293\n",
      "Submitted batch job 1278294\n",
      "Submitted batch job 1278295\n",
      "Submitted batch job 1278296\n"
     ]
    }
   ],
   "source": [
    "!sbatch --job-name=train_minimo --nodelist=node[5] --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=10:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add n_conjectures=200 agent.max_mcts_nodes=50 agent.policy.batch_size=10000 agent.policy.train_iterations=2000 agent.policy.total_iterations=1000 agent.policy.mu=0.01 goals=nat-add-synthetic seed=0\"\n",
    "!sbatch --job-name=train_minimo --nodelist=node[5] --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=10:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add n_conjectures=200 agent.max_mcts_nodes=50 agent.policy.batch_size=10000 agent.policy.train_iterations=2000 agent.policy.total_iterations=1000 agent.policy.mu=0.01 goals=nat-add-synthetic seed=10\"\n",
    "!sbatch --job-name=train_minimo --nodelist=node[5] --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=10:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add n_conjectures=200 agent.max_mcts_nodes=50 agent.policy.batch_size=10000 agent.policy.train_iterations=2000 agent.policy.total_iterations=1000 agent.policy.mu=0.01 goals=nat-add-synthetic seed=15\"\n",
    "!sbatch --job-name=train_minimo --nodelist=node[5] --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=10:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add n_conjectures=200 agent.max_mcts_nodes=50 agent.policy.batch_size=10000 agent.policy.train_iterations=2000 agent.policy.total_iterations=1000 agent.policy.mu=0.01 goals=nat-add-synthetic seed=200\"\n",
    "!sbatch --job-name=train_minimo --nodelist=node[5] --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=10:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add n_conjectures=200 agent.max_mcts_nodes=50 agent.policy.batch_size=10000 agent.policy.train_iterations=2000 agent.policy.total_iterations=1000 agent.policy.mu=0.01 goals=nat-add-synthetic seed=69\"\n",
    "!sbatch --job-name=train_minimo --nodelist=node[6] --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=10:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add n_conjectures=200 agent.max_mcts_nodes=50 agent.policy.batch_size=10000 agent.policy.train_iterations=2000 agent.policy.total_iterations=1000 agent.policy.mu=0.01 goals=nat-add-synthetic seed=420\"\n",
    "!sbatch --job-name=train_minimo --nodelist=node[6] --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=10:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add n_conjectures=200 agent.max_mcts_nodes=50 agent.policy.batch_size=10000 agent.policy.train_iterations=2000 agent.policy.total_iterations=1000 agent.policy.mu=0.01 goals=nat-add-synthetic seed=80085\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "yes variance is much lower (std=0.0025). ok seems like we need to go back to big runs. try the config we had earlier that we used to generate the synth nat-add goal. When we generated the goal, it took the vanilla run 6 iterations. let's hope it finds it again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 1278307\n",
      "Submitted batch job 1278308\n",
      "Submitted batch job 1278309\n",
      "Submitted batch job 1278310\n",
      "Submitted batch job 1278311\n",
      "Submitted batch job 1278312\n",
      "Submitted batch job 1278313\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!sbatch --job-name=train_minimo --nodelist=node[5] --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=10:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add n_conjectures=50 agent.max_mcts_nodes=200 agent.policy.batch_size=10000 agent.policy.train_iterations=500 agent.policy.total_iterations=1000 agent.policy.mu=0 goals=nat-add-synthetic\"\n",
    "!sbatch --job-name=train_minimo --nodelist=node[6] --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=10:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add n_conjectures=50 agent.max_mcts_nodes=200 agent.policy.batch_size=10000 agent.policy.train_iterations=500 agent.policy.total_iterations=1000 agent.policy.mu=1 goals=nat-add-synthetic\"\n",
    "!sbatch --job-name=train_minimo --nodelist=node[6] --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=10:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add n_conjectures=50 agent.max_mcts_nodes=200 agent.policy.batch_size=10000 agent.policy.train_iterations=500 agent.policy.total_iterations=1000 agent.policy.mu=0.1 goals=nat-add-synthetic\"\n",
    "!sbatch --job-name=train_minimo --nodelist=node[5] --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=10:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add n_conjectures=50 agent.max_mcts_nodes=200 agent.policy.batch_size=10000 agent.policy.train_iterations=500 agent.policy.total_iterations=1000 agent.policy.mu=0.05 goals=nat-add-synthetic\"\n",
    "!sbatch --job-name=train_minimo --nodelist=node[5] --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=10:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add n_conjectures=50 agent.max_mcts_nodes=200 agent.policy.batch_size=10000 agent.policy.train_iterations=500 agent.policy.total_iterations=1000 agent.policy.mu=0.01 goals=nat-add-synthetic\"\n",
    "!sbatch --job-name=train_minimo --nodelist=node[5] --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=10:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add n_conjectures=50 agent.max_mcts_nodes=200 agent.policy.batch_size=10000 agent.policy.train_iterations=500 agent.policy.total_iterations=1000 agent.policy.mu=0.005 goals=nat-add-synthetic\"\n",
    "!sbatch --job-name=train_minimo --nodelist=node[5] --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=10:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add n_conjectures=50 agent.max_mcts_nodes=200 agent.policy.batch_size=10000 agent.policy.train_iterations=500 agent.policy.total_iterations=1000 agent.policy.mu=0.001 goals=nat-add-synthetic\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The proved ratio is just way too low. Can we artificially get it up by increasing the percentile of hard (to 50)? (And easy to 70) Since we are already adding 'difficulty' through our goal conditioning, we might be breaking the equilibrium there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 1278322\n"
     ]
    }
   ],
   "source": [
    "!sbatch --job-name=train_minimo --nodelist=node[6] --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=10:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add n_conjectures=50 agent.max_mcts_nodes=200 agent.policy.batch_size=10000 agent.policy.train_iterations=500 agent.policy.total_iterations=1000 agent.policy.mu=0.005 goals=nat-add-synthetic\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "alright so proved ratio went up! The val loss is going down as well! Now lets sweep different values for mu "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 1278364\n",
      "Submitted batch job 1278365\n",
      "Submitted batch job 1278366\n",
      "Submitted batch job 1278367\n",
      "Submitted batch job 1278368\n"
     ]
    }
   ],
   "source": [
    "!sbatch --job-name=train_minimo --nodelist=node[6] --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=10:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add n_conjectures=50 agent.max_mcts_nodes=200 agent.policy.batch_size=10000 agent.policy.train_iterations=500 agent.policy.total_iterations=1000 agent.policy.mu=0.1 goals=nat-add-synthetic\"\n",
    "!sbatch --job-name=train_minimo --nodelist=node[5] --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=10:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add n_conjectures=50 agent.max_mcts_nodes=200 agent.policy.batch_size=10000 agent.policy.train_iterations=500 agent.policy.total_iterations=1000 agent.policy.mu=0.05 goals=nat-add-synthetic\"\n",
    "!sbatch --job-name=train_minimo --nodelist=node[5] --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=10:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add n_conjectures=50 agent.max_mcts_nodes=200 agent.policy.batch_size=10000 agent.policy.train_iterations=500 agent.policy.total_iterations=1000 agent.policy.mu=0.01 goals=nat-add-synthetic\"\n",
    "!sbatch --job-name=train_minimo --nodelist=node[5] --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=10:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add n_conjectures=50 agent.max_mcts_nodes=200 agent.policy.batch_size=10000 agent.policy.train_iterations=500 agent.policy.total_iterations=1000 agent.policy.mu=0.005 goals=nat-add-synthetic\"\n",
    "!sbatch --job-name=train_minimo --nodelist=node[5] --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=10:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add n_conjectures=50 agent.max_mcts_nodes=200 agent.policy.batch_size=10000 agent.policy.train_iterations=500 agent.policy.total_iterations=1000 agent.policy.mu=0.001 goals=nat-add-synthetic\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "okay so it seems like `mu=0.001` is the most promising value out of these runs. Just to confirm that we were not 'lucky', let's try it with different seeds. We expect `mu=0.001` to consistently have lower validation loss as minimo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 1278747\n",
      "Submitted batch job 1278748\n",
      "Submitted batch job 1278749\n",
      "Submitted batch job 1278750\n",
      "Submitted batch job 1278751\n",
      "Submitted batch job 1278752\n",
      "Submitted batch job 1278753\n",
      "Submitted batch job 1278754\n"
     ]
    }
   ],
   "source": [
    "# mu=0.001 with different seeds\n",
    "!sbatch --job-name=train_minimo --nodelist=node[5] --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=5:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add n_conjectures=50 agent.max_mcts_nodes=200 agent.policy.batch_size=10000 difficulty_buckets=[{hard:50},{easy:70},{triv:100}] agent.policy.train_iterations=500 agent.policy.total_iterations=1000 agent.policy.mu=0.001 goals=nat-add-synthetic seed=0\"\n",
    "!sbatch --job-name=train_minimo --nodelist=node[5] --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=5:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add n_conjectures=50 agent.max_mcts_nodes=200 agent.policy.batch_size=10000 difficulty_buckets=[{hard:50},{easy:70},{triv:100}] agent.policy.train_iterations=500 agent.policy.total_iterations=1000 agent.policy.mu=0.001 goals=nat-add-synthetic seed=42\"\n",
    "!sbatch --job-name=train_minimo --nodelist=node[5] --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=5:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add n_conjectures=50 agent.max_mcts_nodes=200 agent.policy.batch_size=10000 difficulty_buckets=[{hard:50},{easy:70},{triv:100}] agent.policy.train_iterations=500 agent.policy.total_iterations=1000 agent.policy.mu=0.001 goals=nat-add-synthetic seed=15\"\n",
    "!sbatch --job-name=train_minimo --nodelist=node[5] --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=5:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add n_conjectures=50 agent.max_mcts_nodes=200 agent.policy.batch_size=10000 difficulty_buckets=[{hard:50},{easy:70},{triv:100}] agent.policy.train_iterations=500 agent.policy.total_iterations=1000 agent.policy.mu=0.001 goals=nat-add-synthetic seed=200\"\n",
    "\n",
    "# vanilla with different seeds\n",
    "!sbatch --job-name=train_minimo --nodelist=node[6] --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=5:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add n_conjectures=50 agent.max_mcts_nodes=200 agent.policy.batch_size=10000 difficulty_buckets=[{hard:50},{easy:70},{triv:100}] agent.policy.train_iterations=500 agent.policy.total_iterations=1000 agent.policy.mu=0 goals=nat-add-synthetic seed=0\"\n",
    "!sbatch --job-name=train_minimo --nodelist=node[6] --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=5:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add n_conjectures=50 agent.max_mcts_nodes=200 agent.policy.batch_size=10000 difficulty_buckets=[{hard:50},{easy:70},{triv:100}] agent.policy.train_iterations=500 agent.policy.total_iterations=1000 agent.policy.mu=0 goals=nat-add-synthetic seed=42\"\n",
    "!sbatch --job-name=train_minimo --nodelist=node[6] --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=5:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add n_conjectures=50 agent.max_mcts_nodes=200 agent.policy.batch_size=10000 difficulty_buckets=[{hard:50},{easy:70},{triv:100}] agent.policy.train_iterations=500 agent.policy.total_iterations=1000 agent.policy.mu=0 goals=nat-add-synthetic seed=15\"\n",
    "!sbatch --job-name=train_minimo --nodelist=node[6] --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=5:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add n_conjectures=50 agent.max_mcts_nodes=200 agent.policy.batch_size=10000 difficulty_buckets=[{hard:50},{easy:70},{triv:100}] agent.policy.train_iterations=500 agent.policy.total_iterations=1000 agent.policy.mu=0 goals=nat-add-synthetic seed=200\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 1279513\n",
      "Submitted batch job 1279514\n",
      "Submitted batch job 1279515\n",
      "Submitted batch job 1279516\n",
      "Submitted batch job 1279517\n",
      "Submitted batch job 1279518\n",
      "Submitted batch job 1279519\n",
      "Submitted batch job 1279520\n"
     ]
    }
   ],
   "source": [
    "# mu=0.001 with different seeds\n",
    "!sbatch --job-name=train_minimo --nodelist=node[5] --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=5:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add n_conjectures=50 agent.max_mcts_nodes=200 agent.policy.batch_size=10000 agent.policy.threshold=0.15 difficulty_buckets=[{hard:50},{easy:70},{triv:100}] agent.policy.train_iterations=500 agent.policy.total_iterations=1000 agent.policy.mu=0.001 goals=nat-add-synthetic seed=0\"\n",
    "!sbatch --job-name=train_minimo --nodelist=node[5] --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=5:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add n_conjectures=50 agent.max_mcts_nodes=200 agent.policy.batch_size=10000 agent.policy.threshold=0.15 difficulty_buckets=[{hard:50},{easy:70},{triv:100}] agent.policy.train_iterations=500 agent.policy.total_iterations=1000 agent.policy.mu=0.001 goals=nat-add-synthetic seed=42\"\n",
    "!sbatch --job-name=train_minimo --nodelist=node[5] --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=5:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add n_conjectures=50 agent.max_mcts_nodes=200 agent.policy.batch_size=10000 agent.policy.threshold=0.15 difficulty_buckets=[{hard:50},{easy:70},{triv:100}] agent.policy.train_iterations=500 agent.policy.total_iterations=1000 agent.policy.mu=0.001 goals=nat-add-synthetic seed=15\"\n",
    "!sbatch --job-name=train_minimo --nodelist=node[5] --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=5:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add n_conjectures=50 agent.max_mcts_nodes=200 agent.policy.batch_size=10000 agent.policy.threshold=0.15 agent.policy.train_iterations=500 agent.policy.total_iterations=1000 agent.policy.mu=0.001 goals=nat-add-synthetic seed=0\"\n",
    "!sbatch --job-name=train_minimo --nodelist=node[5] --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=5:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add n_conjectures=50 agent.max_mcts_nodes=200 agent.policy.batch_size=10000 agent.policy.threshold=0.15 agent.policy.train_iterations=500 agent.policy.total_iterations=1000 agent.policy.mu=0.001 goals=nat-add-synthetic seed=42\"\n",
    "\n",
    "# vanilla with different seeds\n",
    "!sbatch --job-name=train_minimo --nodelist=node[6] --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=5:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add n_conjectures=50 agent.max_mcts_nodes=200 agent.policy.batch_size=10000 agent.policy.train_iterations=500 agent.policy.total_iterations=1000 agent.policy.mu=0 goals=nat-add-synthetic seed=0\"\n",
    "!sbatch --job-name=train_minimo --nodelist=node[6] --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=5:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add n_conjectures=50 agent.max_mcts_nodes=200 agent.policy.batch_size=10000 agent.policy.train_iterations=500 agent.policy.total_iterations=1000 agent.policy.mu=0 goals=nat-add-synthetic seed=42\"\n",
    "!sbatch --job-name=train_minimo --nodelist=node[6] --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=5:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add n_conjectures=50 agent.max_mcts_nodes=200 agent.policy.batch_size=10000 agent.policy.train_iterations=500 agent.policy.total_iterations=1000 agent.policy.mu=0 goals=nat-add-synthetic seed=15\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay so the variance between these runs is immense. However, what we can see is that for the vanilla runs, it is completely random if and when the val loss goes down. It often even goes back up. This makes sense because the directions the model improves into is random. \n",
    "For us 1 out of 3 runs actually got up to a proved_ratio that was above the threshold value of 0.3. Need to investigate why! However the run that did get above the threshold, also eventually had the lowest val_loss out of all the runs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "up until now we have seen that our method works towards the goal while minimo doesn't necessarily. We saw this through the val loss. Now let's use a slightly simpler goal with fewer mcts steps and see if we can see the same results while actually eventually solving the goal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 1278978\n",
      "Submitted batch job 1278979\n",
      "Submitted batch job 1278980\n",
      "Submitted batch job 1278981\n"
     ]
    }
   ],
   "source": [
    "# baseline run\n",
    "!sbatch --job-name=train_minimo --nodelist=node[6] --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=5:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add n_conjectures=50 agent.max_mcts_nodes=100 agent.policy.batch_size=10000 difficulty_buckets=[{hard:50},{easy:70},{triv:100}] agent.policy.train_iterations=500 agent.policy.total_iterations=1000 agent.policy.mu=0 goals=nat-add-easy\"\n",
    "# baseline run with different seed\n",
    "!sbatch --job-name=train_minimo --nodelist=node[6] --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=5:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add n_conjectures=50 agent.max_mcts_nodes=100 agent.policy.batch_size=10000 difficulty_buckets=[{hard:50},{easy:70},{triv:100}] agent.policy.train_iterations=500 agent.policy.total_iterations=1000 agent.policy.mu=0 goals=nat-add-easy seed=42\"\n",
    "\n",
    "# ours with different seeds\n",
    "!sbatch --job-name=train_minimo --nodelist=node[6] --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=5:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add n_conjectures=50 agent.max_mcts_nodes=100 agent.policy.batch_size=10000 difficulty_buckets=[{hard:50},{easy:70},{triv:100}] agent.policy.train_iterations=500 agent.policy.total_iterations=1000 agent.policy.mu=0.001 goals=nat-add-easy\"\n",
    "!sbatch --job-name=train_minimo --nodelist=node[6] --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=5:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add n_conjectures=50 agent.max_mcts_nodes=100 agent.policy.batch_size=10000 difficulty_buckets=[{hard:50},{easy:70},{triv:100}] agent.policy.train_iterations=500 agent.policy.total_iterations=1000 agent.policy.mu=0.001 goals=nat-add-easy seed=42\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and run them with even fewer mcts steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 1278982\n",
      "Submitted batch job 1278983\n",
      "Submitted batch job 1278984\n",
      "Submitted batch job 1278985\n"
     ]
    }
   ],
   "source": [
    "# baseline run\n",
    "!sbatch --job-name=train_minimo --nodelist=node[5] --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=5:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add n_conjectures=50 agent.max_mcts_nodes=50 agent.policy.batch_size=10000 difficulty_buckets=[{hard:50},{easy:70},{triv:100}] agent.policy.train_iterations=500 agent.policy.total_iterations=1000 agent.policy.mu=0 goals=nat-add-easy\"\n",
    "# baseline run with different seed\n",
    "!sbatch --job-name=train_minimo --nodelist=node[5] --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=5:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add n_conjectures=50 agent.max_mcts_nodes=50 agent.policy.batch_size=10000 difficulty_buckets=[{hard:50},{easy:70},{triv:100}] agent.policy.train_iterations=500 agent.policy.total_iterations=1000 agent.policy.mu=0 goals=nat-add-easy seed=42\"\n",
    "\n",
    "# ours with different seeds\n",
    "!sbatch --job-name=train_minimo --nodelist=node[5] --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=5:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add n_conjectures=50 agent.max_mcts_nodes=50 agent.policy.batch_size=10000 difficulty_buckets=[{hard:50},{easy:70},{triv:100}] agent.policy.train_iterations=500 agent.policy.total_iterations=1000 agent.policy.mu=0.001 goals=nat-add-easy\"\n",
    "!sbatch --job-name=train_minimo --nodelist=node[5] --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=5:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add n_conjectures=50 agent.max_mcts_nodes=50 agent.policy.batch_size=10000 difficulty_buckets=[{hard:50},{easy:70},{triv:100}] agent.policy.train_iterations=500 agent.policy.total_iterations=1000 agent.policy.mu=0.001 goals=nat-add-easy seed=42\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 1279192\n",
      "Submitted batch job 1279193\n"
     ]
    }
   ],
   "source": [
    "!sbatch --job-name=train_minimo --nodelist=node[6] --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=5:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add n_conjectures=50 agent.max_mcts_nodes=150 agent.policy.batch_size=10000 difficulty_buckets=[{hard:50},{easy:70},{triv:100}] agent.policy.train_iterations=500 agent.policy.total_iterations=1000 agent.policy.mu=0 goals=nat-add-easy\"\n",
    "!sbatch --job-name=train_minimo --nodelist=node[5] --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=5:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add n_conjectures=50 agent.max_mcts_nodes=150 agent.policy.batch_size=10000 difficulty_buckets=[{hard:50},{easy:70},{triv:100}] agent.policy.train_iterations=500 agent.policy.total_iterations=1000 agent.policy.mu=0 goals=nat-add-easy\"\n",
    "!sbatch --job-name=train_minimo --nodelist=node[5] --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=5:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add n_conjectures=50 agent.max_mcts_nodes=150 agent.policy.batch_size=10000 difficulty_buckets=[{hard:50},{easy:70},{triv:100}] agent.policy.train_iterations=500 agent.policy.total_iterations=1000 agent.policy.mu=0 goals=nat-add-easy seed=42\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare sweep for franz\n",
    "!python learning/bootstrap.py theory=nat-add n_conjectures=50 agent.max_mcts_nodes=150 agent.policy.batch_size=10000 difficulty_buckets=[{hard:50},{easy:70},{triv:100}] agent.policy.train_iterations=500 agent.policy.total_iterations=1000 agent.policy.mu=0 goals=nat-add-synthetic agent.policy.threshold=0\n",
    "!python learning/bootstrap.py theory=nat-add n_conjectures=50 agent.max_mcts_nodes=150 agent.policy.batch_size=10000 difficulty_buckets=[{hard:50},{easy:70},{triv:100}] agent.policy.train_iterations=500 agent.policy.total_iterations=1000 agent.policy.mu=1e-1 goals=nat-add-synthetic agent.policy.threshold=0\n",
    "!python learning/bootstrap.py theory=nat-add n_conjectures=50 agent.max_mcts_nodes=150 agent.policy.batch_size=10000 difficulty_buckets=[{hard:50},{easy:70},{triv:100}] agent.policy.train_iterations=500 agent.policy.total_iterations=1000 agent.policy.mu=1e-2 goals=nat-add-synthetic agent.policy.threshold=0\n",
    "!python learning/bootstrap.py theory=nat-add n_conjectures=50 agent.max_mcts_nodes=150 agent.policy.batch_size=10000 difficulty_buckets=[{hard:50},{easy:70},{triv:100}] agent.policy.train_iterations=500 agent.policy.total_iterations=1000 agent.policy.mu=1e-3 goals=nat-add-synthetic agent.policy.threshold=0\n",
    "!python learning/bootstrap.py theory=nat-add n_conjectures=50 agent.max_mcts_nodes=150 agent.policy.batch_size=10000 difficulty_buckets=[{hard:50},{easy:70},{triv:100}] agent.policy.train_iterations=500 agent.policy.total_iterations=1000 agent.policy.mu=1e-4 goals=nat-add-synthetic agent.policy.threshold=0\n",
    "!python learning/bootstrap.py theory=nat-add n_conjectures=50 agent.max_mcts_nodes=150 agent.policy.batch_size=10000 difficulty_buckets=[{hard:50},{easy:70},{triv:100}] agent.policy.train_iterations=500 agent.policy.total_iterations=1000 agent.policy.mu=1e-5 goals=nat-add-synthetic agent.policy.threshold=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 1279228\n",
      "Submitted batch job 1279229\n",
      "Submitted batch job 1279230\n",
      "Submitted batch job 1279231\n"
     ]
    }
   ],
   "source": [
    "!sbatch --job-name=train_minimo --nodelist=node[6] --cpus-per-task=4 --mem=50G --gres=gpu:1,vram=12G --time=5:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add n_conjectures=50 agent.max_mcts_nodes=150 agent.policy.batch_size=10000 difficulty_buckets=[{hard:50},{easy:70},{triv:100}] agent.policy.train_iterations=500 agent.policy.total_iterations=1000 agent.policy.mu=1e-4 goals=nat-add-easy agent.policy.threshold=0\"\n",
    "!sbatch --job-name=train_minimo --nodelist=node[6] --cpus-per-task=4 --mem=50G --gres=gpu:1,vram=12G --time=5:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add n_conjectures=50 agent.max_mcts_nodes=150 agent.policy.batch_size=10000 difficulty_buckets=[{hard:50},{easy:70},{triv:100}] agent.policy.train_iterations=500 agent.policy.total_iterations=1000 agent.policy.mu=1e-2 goals=nat-add-easy agent.policy.threshold=0\"\n",
    "!sbatch --job-name=train_minimo --nodelist=node[5] --cpus-per-task=4 --mem=50G --gres=gpu:1,vram=12G --time=5:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add n_conjectures=50 agent.max_mcts_nodes=150 agent.policy.batch_size=10000 difficulty_buckets=[{hard:50},{easy:70},{triv:100}] agent.policy.train_iterations=500 agent.policy.total_iterations=1000 agent.policy.mu=1e-3 goals=nat-add-easy agent.policy.threshold=0\"\n",
    "!sbatch --job-name=train_minimo --nodelist=node[5] --cpus-per-task=4 --mem=50G --gres=gpu:1,vram=12G --time=5:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add n_conjectures=50 agent.max_mcts_nodes=150 agent.policy.batch_size=10000 difficulty_buckets=[{hard:50},{easy:70},{triv:100}] agent.policy.train_iterations=500 agent.policy.total_iterations=1000 agent.policy.mu=1e-1 goals=nat-add-easy agent.policy.threshold=0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 1279387\n",
      "Submitted batch job 1279388\n",
      "Submitted batch job 1279389\n",
      "Submitted batch job 1279390\n"
     ]
    }
   ],
   "source": [
    "!sbatch --job-name=train_minimo --nodelist=node[6] --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=5:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add n_conjectures=15 agent.max_mcts_nodes=50 agent.policy.batch_size=10000 agent.policy.train_iterations=150 agent.policy.total_iterations=1000 agent.policy.mu=1e-1 goals=nat-add-zeros agent.policy.threshold=0\"\n",
    "!sbatch --job-name=train_minimo --nodelist=node[6] --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=5:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add n_conjectures=15 agent.max_mcts_nodes=50 agent.policy.batch_size=10000 agent.policy.train_iterations=150 agent.policy.total_iterations=1000 agent.policy.mu=1e-2 goals=nat-add-zeros agent.policy.threshold=0\"\n",
    "!sbatch --job-name=train_minimo --nodelist=node[5] --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=5:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add n_conjectures=15 agent.max_mcts_nodes=50 agent.policy.batch_size=10000 agent.policy.train_iterations=150 agent.policy.total_iterations=1000 agent.policy.mu=1e-3 goals=nat-add-zeros agent.policy.threshold=0\"\n",
    "!sbatch --job-name=train_minimo --nodelist=node[5] --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=5:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add n_conjectures=15 agent.max_mcts_nodes=50 agent.policy.batch_size=10000 agent.policy.train_iterations=150 agent.policy.total_iterations=1000 agent.policy.mu=1e-4 goals=nat-add-zeros agent.policy.threshold=0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [GAINING BACK CONTROL OF THE EXPERIMENTS!]\n",
    "\n",
    "ok some time passed and i was doing some random stuff. for now you don't have to understand the jump from last cell to the upcoming one. i will write up my thoughts on this later. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nat-add-zeros\n",
    "\n",
    "i created a new goal by letting minimo run for many iterations with 15 conjectures each and 50 mcts steps. the aim was to find an easier goal which is attainable with reasonable compute.\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"name\": \"nat-add-zeros\",\n",
    "    \"theorem\": \"(= (+ z (s z)) (s (+ z (+ z z))))\",\n",
    "    \"solution\": [\n",
    "        \"S: G=1\\nState: {  }\\nGoal: (= (+ z (+ z (+ (+ z z) z))) (+ z (+ z (+ (+ z z) z))))\\n\\nA: c eq_refl\\n???Y\",\n",
    "        \"S: G=1\\nState: {  }\\nGoal: (= (+ z (+ z (+ (+ z z) z))) (+ z (+ z (+ (+ z z) z))))\\nconstruct eq_refl\\nA: => (= (+ z (+ z (+ (+ z z) z))) (+ z (+ z (+ (+ z z) z)))).\\n???Y\",\n",
    "        \"S: <solved>\\n???Y\",\n",
    "        \"S: G=1\\nState: {  }\\nGoal: (= (+ z (+ z (+ (+ z z) z))) (+ z (+ z (+ (+ z z) z))))\\nconstruct eq_refl\\n???Y\",\n",
    "        \"S: G=1\\nState: {  }\\nGoal: (= (+ z (+ z (+ (+ z z) z))) (+ z (+ z (+ (+ z z) z))))\\n\\n???Y\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "```\n",
    "In pseudo-code this would be:\n",
    "`0 + ++0 == ++(0 + (0 + 0))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is valid python code btw\n",
    "0 + ++0 == ++(0 + (0 + 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now to solve this goal we will increase the mcts steps. after talking to claude it also makes sense to decrease the number of expansions. this makes sense as the exploration was just unproportionally high this way. let's try to solve the goal with the earliers config in mind\n",
    "\n",
    "```yaml\n",
    "n_conjectures: 50\n",
    "max_mcts_nodes: 200\n",
    "mu: 1e-3\n",
    "threshold: 0.2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python bootstrap.py theory=nat-add n_conjectures=50 agent.max_mcts_nodes=200 agent.expansions=500 agent.policy.batch_size=10000 agent.policy.train_iterations=500 agent.policy.total_iterations=50 agent.policy.mu=0.001 difficulty_buckets=[{hard:20},{easy:50},{triv:100}] goals=nat-add-zeros agent.policy.threshold=0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "holy shit, this worked! this run solved the goal-problem in 12 iterations. i think we are onto something. let's see if we can get to our goal by just setting the threshold to 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python bootstrap.py theory=nat-add n_conjectures=50 agent.max_mcts_nodes=200 agent.expansions=500 agent.policy.batch_size=10000 agent.policy.train_iterations=500 agent.policy.total_iterations=50 agent.policy.mu=0.001 difficulty_buckets=[{hard:20},{easy:50},{triv:100}] goals=nat-add-zeros agent.policy.threshold=0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and now a vanilla one as a baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python bootstrap.py theory=nat-add n_conjectures=50 agent.max_mcts_nodes=200 agent.expansions=500 agent.policy.batch_size=10000 agent.policy.train_iterations=500 agent.policy.total_iterations=50 agent.policy.mu=0 difficulty_buckets=[{hard:20},{easy:50},{triv:100}] goals=nat-add-zeros agent.policy.threshold=0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ok nice this one also solved the goal, but after 14 iterations!!! need to dig into this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 1279534\n",
      "Submitted batch job 1279535\n",
      "Submitted batch job 1279536\n",
      "Submitted batch job 1279537\n"
     ]
    }
   ],
   "source": [
    "# bootstrap.py theory=nat-add n_conjectures=50 agent.max_mcts_nodes=200 agent.expansions=500 agent.policy.batch_size=10000 agent.policy.train_iterations=500 agent.policy.total_iterations=50 agent.policy.mu=0.001 difficulty_buckets=[{hard:20},{easy:50},{triv:100}] goals=nat-add-zeros agent.policy.threshold=0.2\n",
    "!sbatch --job-name=train_minimo --nodelist=node[6] --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=5:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add n_conjectures=50 agent.max_mcts_nodes=200 agent.expansions=500 agent.policy.batch_size=10000 agent.policy.train_iterations=500 agent.policy.total_iterations=50 agent.policy.mu=0.001 difficulty_buckets=[{hard:20},{easy:50},{triv:100}] goals=nat-add-zeros agent.policy.threshold=0.1\"\n",
    "!sbatch --job-name=train_minimo --nodelist=node[6] --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=5:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add n_conjectures=50 agent.max_mcts_nodes=200 agent.expansions=500 agent.policy.batch_size=10000 agent.policy.train_iterations=500 agent.policy.total_iterations=50 agent.policy.mu=0.001 difficulty_buckets=[{hard:20},{easy:50},{triv:100}] goals=nat-add-zeros agent.policy.threshold=0\"\n",
    "!sbatch --job-name=train_minimo --nodelist=node[5] --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=5:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add n_conjectures=50 agent.max_mcts_nodes=200 agent.expansions=500 agent.policy.batch_size=10000 agent.policy.train_iterations=500 agent.policy.total_iterations=50 agent.policy.mu=0.001 difficulty_buckets=[{hard:50},{easy:70},{triv:100}] goals=nat-add-zeros agent.policy.threshold=0.1\"\n",
    "!sbatch --job-name=train_minimo --nodelist=node[5] --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=5:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add n_conjectures=50 agent.max_mcts_nodes=200 agent.expansions=500 agent.policy.batch_size=10000 agent.policy.train_iterations=500 agent.policy.total_iterations=50 agent.policy.mu=0.001 difficulty_buckets=[{hard:50},{easy:70},{triv:100}] goals=nat-add-zeros agent.policy.threshold=0\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minimo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
